{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime\n",
    "import os\n",
    "import psycopg2.extras\n",
    "from psycopg2.pool import ThreadedConnectionPool\n",
    "import psycopg2 as pg\n",
    "import psycopg2  \n",
    "import sys\n",
    "import warnings \n",
    "from io import StringIO\n",
    "import pyreadstat\n",
    "\n",
    "import boto3 \n",
    "\n",
    "#%matplotlib inline  \n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('float_format', '{:f}'.format)\n",
    "pd.set_option('display.precision',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [

   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource = boto3.resource('s3', aws_access_key_id=AK_KC, aws_secret_access_key=SK_KC)\n",
    "s3_client = boto3.client('s3', aws_access_key_id=AK_KC, aws_secret_access_key=SK_KC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bucket in s3_resource.buckets.all():\n",
    "    print(bucket.name)\n",
    "    bucketname=bucket.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_files(Prefix):\n",
    "    global max_lastdate_file\n",
    "    try:\n",
    "        my_bucket = s3_resource.Bucket(bucketname)\n",
    "        lst_ficheros=[]\n",
    "        max_lastdate_file=[]\n",
    "        for my_bucket_object in my_bucket.objects.filter(Prefix= Prefix):\n",
    "            file_datemodified=my_bucket_object.last_modified.replace(tzinfo = None)\n",
    "            if file_datemodified>fecha_max:\n",
    "                if my_bucket_object.key.endswith('gz'):\n",
    "                    lst_ficheros.append(my_bucket_object.key)\n",
    "                    file=[my_bucket_object.key, file_datemodified]\n",
    "                    max_lastdate_file.append(file)\n",
    "        return max_lastdate_file# lst_ficheros   \n",
    "    except Exception as e:\n",
    "        print(f\"Error al obtener la lista de archivos: {e}\")\n",
    "        return []    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hora_inicio=(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_with_extension(folder_path, extension):\n",
    "    file_list = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(extension):\n",
    "                file_list.append(os.path.join(root, file))\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['G:\\\\Mi Unidad\\\\Clientes\\\\Kids Corp\\\\2.Proyectos\\\\2.Paneles Encuestas\\\\1.Inputs\\\\ASKDAR_251323_Ago23_w5_20230914_Latam+Usa_pp3.sav',\n",
       " 'G:\\\\Mi Unidad\\\\Clientes\\\\Kids Corp\\\\2.Proyectos\\\\2.Paneles Encuestas\\\\1.Inputs\\\\ASKDAR_251323_Nov23_w7_20231227_Latam+Usa_pp.sav',\n",
       " 'G:\\\\Mi Unidad\\\\Clientes\\\\Kids Corp\\\\2.Proyectos\\\\2.Paneles Encuestas\\\\1.Inputs\\\\ASKDAR_251323_Oct23_w6_20231130_Latam+Usa_pp.sav']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path = r'G:\\Mi Unidad\\Clientes\\Kids Corp\\2.Proyectos\\2.Paneles Encuestas\\1.Inputs'\n",
    "#folder_path = r'G:\\My Drive\\Clientes\\Kids Corp\\2.Proyectos\\2.Paneles Encuestas\\1.Inputs'\n",
    "#folder_path=os.getcwd()\n",
    "extension = '.sav'\n",
    "\n",
    "sav_files = get_files_with_extension(folder_path, extension)\n",
    "\n",
    "sav_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de la conexión a PostgreSQL\n",
    "nombre_usuario = 'dbpmconsulting'\n",
    "contrasena = 'Dbpm2017'\n",
    "nombre_base_datos = 'postgres'\n",
    "host = 'scrapping.ciekeymv912n.us-west-2.rds.amazonaws.com'\n",
    "puerto = '5432'\n",
    "schema='stg'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_cursor():\n",
    "    global conn, cursor\n",
    "    conn = psycopg2.connect(host=host, port=puerto, database=nombre_base_datos, user=nombre_usuario, password=contrasena)\n",
    "    conn.autocommit = True\n",
    "    cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lista_filtrada(df_resultado, lista):\n",
    "    # Filtrar las columnas del DataFrame\n",
    "    lista_filtrada = [col for col in df_resultado.columns if col in lista]\n",
    "\n",
    "    return lista_filtrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a connect function for PostgreSQL database server\n",
    "def connect():\n",
    "    conn = None\n",
    "    try:\n",
    "        print('Connecting to the PostgreSQL...........')\n",
    "        conn = psycopg2.connect(host=host, port=puerto, database=nombre_base_datos, user=nombre_usuario, password=contrasena)\n",
    "        conn.autocommit = True\n",
    "        print(\"Connection successfully..................\")\n",
    "        \n",
    "    except OperationalError as err:\n",
    "        # passing exception to function\n",
    "        show_psycopg2_exception(err)        \n",
    "        # set the connection to 'None' in case of error\n",
    "        conn = None\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that handles and parses psycopg2 exceptions\n",
    "def show_psycopg2_exception(err):\n",
    "    # get details about the exception\n",
    "    err_type, err_obj, traceback = sys.exc_info()    \n",
    "    # get the line number when exception occured\n",
    "    line_n = traceback.tb_lineno    \n",
    "    # print the connect() error\n",
    "    print (\"\\npsycopg2 ERROR:\", err, \"on line number:\", line_n)\n",
    "    print (\"psycopg2 traceback:\", traceback, \"-- type:\", err_type) \n",
    "    # psycopg2 extensions.Diagnostics object attribute\n",
    "    print (\"\\nextensions.Diagnostics:\", err.diag)    \n",
    "    # print the pgcode and pgerror exceptions\n",
    "    print (\"pgerror:\", err.pgerror)\n",
    "    print (\"pgcode:\", err.pgcode, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function using copy_from() with StringIO to insert the dataframe\n",
    "def copy_from_dataFile_StringIO(conn, datafrm, table, col_orden):\n",
    "    call_cursor()\n",
    "  # save dataframe to an in memory buffer\n",
    "    buffer = StringIO()\n",
    "    datafrm[col_orden].to_csv(buffer, header=False, index = False, sep=\"|\", na_rep='NaN')\n",
    "    buffer.seek(0)\n",
    "    #conn=connect()\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SET search_path TO \" + schema)\n",
    "    try:\n",
    "        cursor.copy_from(buffer, table, sep=\"|\")\n",
    "        print(\"Data inserted using copy_from_datafile_StringIO() successfully....\")\n",
    "    except (Exception, psycopg2.DatabaseError) as err:\n",
    "        # pass exception to function\n",
    "        show_psycopg2_exception(err)\n",
    "        cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_upsert(fn_upsert):\n",
    "    try:\n",
    "        cursor.callproc(fn_upsert)\n",
    "    except (Exception, psycopg2.DatabaseError) as err:\n",
    "        # pass exception to function\n",
    "        show_psycopg2_exception(err)\n",
    "        cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completa_cols_resultados(df_resultado, lista):\n",
    "    pk=lista.copy()\n",
    "    for columna in pk:\n",
    "        # Verificar si la columna no existe en el DataFrame\n",
    "        if columna not in df_resultado.columns:\n",
    "            # Crear la nueva columna y asignarle un valor predeterminado (por ejemplo, None)\n",
    "            df_resultado[columna] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesamiento de la información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16615, 14612)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Lectura del Fichero SPSS \n",
    "# Carga en DataFrame y formato columnas\n",
    "dataframes = []\n",
    "meta_lst = []\n",
    "for path in sav_files:\n",
    "    df_panel, meta = pyreadstat.read_sav(path)\n",
    "    df_panel.columns=df_panel.columns.str.lower()\n",
    "    df_panel['respondentid']= df_panel['respondentid'].astype(np.int64)\n",
    "    df_panel.columns = df_panel.columns.str.replace(' ', '_')\n",
    "    df_panel.columns = df_panel.columns.str.lower()   \n",
    "    df_panel['file']=os.path.basename(path)\n",
    "    dataframes.append(df_panel)\n",
    "    meta_lst.append(meta)\n",
    "\n",
    "df_panel_final = pd.concat(dataframes, ignore_index=True)\n",
    "df_panel_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_panel_bkp=df_panel_final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_panel_final=df_panel_bkp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define las listas de campos por dimension \n",
    "\n",
    "\n",
    "lst_pk_respondent=['key','respondentid']\n",
    "lst_new_vars=['nse','zona','zona2']\n",
    "lst_nse=['nse_mx','nse_br', 'nse_ar', 'nse_cl', 'nse_pe', 'nse_co']\n",
    "lst_zona=['zona_mx','zona_br', 'zona_ar', 'zona_cl', 'zona_pe', 'zona_co']\n",
    "lst_zona2=['zona2_mx','zona2_br', 'zona2_ar']\n",
    "lst_sociodemograficas=['codpanelista','pais','sexo','edad','edadr', 'f05s','f05e']\n",
    "lst_catalog=['key', 'respondentid', 'variable', 'valor', 'file']\n",
    "lst_panel_ola_cuesti=['ola','cuesti','responsedate','endtime','duration','status','type','device','ponderado210609']\n",
    "\n",
    "#Compila las variables sociodemográficas para el DF\n",
    "lst_dim_demograficas=list(sum(zip(lst_pk_respondent+ lst_sociodemograficas+ lst_new_vars), ()))\n",
    "\n",
    "#Compila las variables sociodemográficas que luego serán eliminadas\n",
    "lst_black=list(sum(zip(lst_new_vars+lst_nse+ lst_zona+ lst_zona2+lst_sociodemograficas), ()))\n",
    "\n",
    "# Defino la variables de cabecera y genero DF\n",
    "\n",
    "lst_cabecera=list(sum(zip(lst_pk_respondent+ lst_panel_ola_cuesti+['file']), ()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Base Sociodemográfica\n",
    "# Agrupa variables sociodemográficas \n",
    "# Genera DF Sociodemográfico por Respondent-Key]\n",
    "# Elimina las variables del Panel\n",
    "\n",
    "\n",
    "#Genera las variables agrupadas\n",
    "df_panel_final[lst_new_vars[0]] = df_panel_final[lst_nse].bfill(axis=1).iloc[:, 0]\n",
    "df_panel_final[lst_new_vars[1]] = df_panel_final[lst_zona].bfill(axis=1).iloc[:, 0]\n",
    "df_panel_final[lst_new_vars[2]] = df_panel_final[lst_zona2].bfill(axis=1).iloc[:, 0]\n",
    "\n",
    "#Genera el DF de variables sociodemográficas por Respondentid - KEY\n",
    "df_socioeconomico=df_panel_final[lst_dim_demograficas].drop_duplicates()\n",
    "\n",
    "#Elimina las variables sociodemográficas del panel\n",
    "df_panel_final.drop(columns=lst_black, inplace=True)\n",
    "\n",
    "## Genero Cabecera de Panel\n",
    "# Defino la variables de cabecera y genero DF\n",
    "\n",
    "df_panel_cabecera=df_panel_final[lst_cabecera]\n",
    "df_panel_cabecera['responsedate']=df_panel_cabecera['responsedate'].replace(\"NaN\", np.NaN)\n",
    "df_panel_cabecera['endtime']=df_panel_cabecera['endtime'].replace(\"NaN\", np.NaN)\n",
    "df_panel_cabecera['endtime'] = pd.to_datetime(df_panel_cabecera['endtime'], errors='coerce')\n",
    "df_panel_cabecera['responsedate'] = pd.to_datetime(df_panel_cabecera['responsedate'], errors='coerce')\n",
    "df_panel_cabecera['endtime'] = df_panel_cabecera['endtime'].fillna(df_panel_cabecera['responsedate'])\n",
    "df_panel_cabecera['responsedate'] = df_panel_cabecera['responsedate'].fillna(df_panel_cabecera['endtime'])\n",
    "\n",
    "#Elimina las variables ola_panel\n",
    "lst_panel_ola_cuesti.remove('ola')\n",
    "df_panel_final.drop(columns=lst_panel_ola_cuesti, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generacion de fact Olas\n",
    "def fn_denormalizacion(df):\n",
    "\n",
    "    #Denormalización de Variables \n",
    "    df_panel_melt=pd.melt(df, id_vars=['key','respondentid','file'], var_name='variable', value_name='valor')\n",
    "    ## Limpieza y filtrado de campos nulos\n",
    "    df_panel_melt['valor']=df_panel_melt['valor'].replace(\".\", '')\n",
    "    df_panel_melt['valor']=df_panel_melt['valor'].replace(\"NaN\", np.NaN)\n",
    "    df_panel_melt=df_panel_melt[df_panel_melt['valor']!='']\n",
    "    df_panel_melt=df_panel_melt.dropna(subset=['valor'])\n",
    "    df_panel_melt=df_panel_melt[df_panel_melt['variable']!='ola']\n",
    "    return df_panel_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst=list(df_panel_final['ola'].drop_duplicates())\n",
    "dataframes = []\n",
    "for i in lst:\n",
    "    df=df_panel_final[df_panel_final['ola']==i]\n",
    "    df_panel_melt_tmp=fn_denormalizacion(df)\n",
    "    dataframes.append(df_panel_melt_tmp)\n",
    "    print(i)\n",
    "df_panel_melt_final = pd.concat(dataframes, ignore_index=True)\n",
    "df_panel_melt_final['valor'] = df_panel_melt_final['valor'].astype(str)\n",
    "df_panel_melt_final['valor']=df_panel_melt_final['valor'].str.replace('|', '-') \n",
    "print(df_panel_melt_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Insertar \n",
    "lst_df=[[df_socioeconomico,lst_dim_demograficas, 'df_sociodemografico', 'stg.fn_df_sociodemografico'],\n",
    "        [df_panel_cabecera,lst_cabecera, 'df_panel_cabecera','stg.fn_df_panel_cabecera'] ,\n",
    "        [df_panel_melt_final,lst_catalog,'df_panel','stg.fn_df_panel']        \n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    call_cursor()\n",
    "    for e in range(len(lst_df)):\n",
    "        df_resultado=lst_df[e][0]\n",
    "        #df_resultado=df_resultado.iloc[:150000]\n",
    "        lst_filtrada_eventos=lista_filtrada(df_resultado, lst_df[e][1])\n",
    "        completa_cols_resultados(df_resultado, lst_df[e][1])\n",
    "\n",
    "        ## Itera el DF\n",
    "        # Separar el DataFrame en partes de 500,000 registros\n",
    "        chunk_size = 200000\n",
    "        num_chunks = -(-len(df_resultado) // chunk_size)  # Redondeo hacia arriba para calcular el número de partes\n",
    "\n",
    "        # Exportar cada parte del DataFrame a un archivo de texto\n",
    "        for i in range(num_chunks):\n",
    "            hora_inicio1=(datetime.now())\n",
    "            start_idx = i * chunk_size\n",
    "            end_idx = min((i + 1) * chunk_size, len(df_resultado))\n",
    "            df_chunk = df_resultado.iloc[start_idx:end_idx]\n",
    "            copy_from_dataFile_StringIO(conn, df_chunk, lst_df[e][2],lst_df[e][1])\n",
    "            print(i+1,' de ', num_chunks, ' ',(i+1)/num_chunks,'%')\n",
    "\n",
    "        #Actualiza ambiente ODS    \n",
    "        #call_upsert(lst_df[e][3])   \n",
    "        print('Fin de upsert')      \n",
    "             \n",
    "except Exception as e:\n",
    "    print(f\"Error al procesar el archivo {i}: {e}\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hora_fin=(datetime.now())\n",
    "delta=(hora_fin-hora_inicio).total_seconds()\n",
    "print(hora_inicio,'-',hora_fin,'-', delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesamiento de la Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_df(data_dict):\n",
    "    df_temp=pd.DataFrame.from_dict(data_dict, orient='index',columns=['valor'])\n",
    "    df_temp['valor']= df_temp['valor'].str.lower()\n",
    "    df_temp.index = df_temp.index.str.lower()\n",
    "    df_temp['clave'] = df_temp.index\n",
    "    df_temp.reset_index(drop=True, inplace=True)\n",
    "    return df_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_dict_redstat_type=[]\n",
    "lst_dict_cols_names=[]\n",
    "lst_dict_labels_type=[]\n",
    "lst_dict_var2label=[]\n",
    "lst_dict_var_opts=[]\n",
    "for i in range(len(sav_files)):\n",
    "    file=os.path.basename(sav_files[i])\n",
    "\n",
    "    tmp=meta_lst[i].readstat_variable_types\n",
    "    tmp=dict_to_df(tmp)\n",
    "    tmp['file']=file\n",
    "    lst_dict_redstat_type.append(tmp)\n",
    "\n",
    "    tmp=meta_lst[i].column_names_to_labels\n",
    "    tmp=dict_to_df(tmp)\n",
    "    tmp['file']=file\n",
    "    lst_dict_cols_names.append(tmp)\n",
    "\n",
    "    tmp=meta_lst[i].original_variable_types\n",
    "    tmp=dict_to_df(tmp)\n",
    "    tmp['file']=file\n",
    "    lst_dict_labels_type.append(tmp)\n",
    "\n",
    "    tmp=meta_lst[i].variable_to_label\n",
    "    tmp=dict_to_df(tmp)\n",
    "    tmp['file']=file\n",
    "    lst_dict_var2label.append(tmp)\n",
    "\n",
    "    tmp=meta_lst[i].variable_value_labels\n",
    "    lista_tuplas = [(k, sk, v) for k, subdict in tmp.items() for sk, v in subdict.items()]\n",
    "\n",
    "    tmp = pd.DataFrame(lista_tuplas, columns=['pregunta', 'id_respuesta', 'respuesta'])\n",
    "    tmp['id_respuesta']= tmp['id_respuesta'].astype(np.int64)\n",
    "    tmp['pregunta']= tmp['pregunta'].str.lower()\n",
    "    tmp['respuesta']= tmp['respuesta'].str.lower()    \n",
    "    tmp['file']=file\n",
    "    lst_dict_var_opts.append(tmp)\n",
    "\n",
    "df_panel_final = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "df_dict_redstat_type= pd.concat(lst_dict_redstat_type, ignore_index=True)\n",
    "df_dict_cols_names= pd.concat(lst_dict_cols_names, ignore_index=True)\n",
    "df_dict_labels_type= pd.concat(lst_dict_labels_type, ignore_index=True)\n",
    "df_dict_var2label= pd.concat(lst_dict_var2label, ignore_index=True)\n",
    "df_catalogo_respuestas= pd.concat(lst_dict_var_opts, ignore_index=True)\n",
    "print(df_catalogo_respuestas.shape)\n",
    "#lst_meta_dicts=[dict_cols_names,dict_labels_type,dict_redstat_type,dict_var2label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata=pd.merge(df_catalogo_respuestas, df_dict_cols_names, left_on=['pregunta','file'], right_on=['clave','file'], how='left').drop(columns='clave')\n",
    "df_metadata = df_metadata.rename(columns={'valor': 'texto_pregunta'})\n",
    "\n",
    "df_metadata=pd.merge(df_metadata, df_dict_labels_type, left_on=['pregunta','file'], right_on=['clave','file'], how='left').drop(columns='clave')\n",
    "df_metadata = df_metadata.rename(columns={'valor': 'label_type'})\n",
    "\n",
    "df_metadata=pd.merge(df_metadata, df_dict_redstat_type, left_on=['pregunta','file'], right_on=['clave','file'], how='left').drop(columns='clave')\n",
    "df_metadata = df_metadata.rename(columns={'valor': 'var_format_type'})\n",
    "\n",
    "df_metadata=pd.merge(df_metadata, df_dict_var2label, left_on=['pregunta','file'], right_on=['clave','file'], how='left').drop(columns='clave')\n",
    "df_metadata = df_metadata.rename(columns={'valor': 'label'})\n",
    "\n",
    "df_metadata[['modulo', 'id_pregunta']] = df_metadata['pregunta'].str.split('#', n=1, expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata.to_excel('fichero_metadata_full.xlsx',index=False)\n",
    "print('export metadata')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lst_dict_var_opts=[]\n",
    "for i in range(len(sav_files)):\n",
    "    file=os.path.basename(sav_files[i])\n",
    "    tmp_dict_var_opts=meta_lst[i].variable_value_labels\n",
    "    lista_tuplas = [(k, sk, v) for k, subdict in tmp_dict_var_opts.items() for sk, v in subdict.items()]\n",
    "\n",
    "    tmp_dict_var_opts = pd.DataFrame(lista_tuplas, columns=['pregunta', 'id_respuesta', 'respuesta'])\n",
    "    tmp_dict_var_opts['id_respuesta']= tmp_dict_var_opts['id_respuesta'].astype(np.int64)\n",
    "    tmp_dict_var_opts['pregunta']= tmp_dict_var_opts['pregunta'].str.lower()\n",
    "    tmp_dict_var_opts['respuesta']= tmp_dict_var_opts['respuesta'].str.lower()    \n",
    "    tmp_dict_var_opts['file']=file\n",
    "    lst_dict_var_opts.append(tmp_dict_var_opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tmp_dict_var_opts=meta_lst[1].variable_value_labels\n",
    "for k, subdict in tmp_dict_var_opts.items():\n",
    "    print(k,' ','subdict') \n",
    "    for sk, v in subdict.items():\n",
    "        print(sk,' ',v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata de listado de respuestas por variable\n",
    "\n",
    "## Catalogo de Valores posibles en cada uan de las respuestas\n",
    "# Convertir el diccionario en una lista de tuplas (clave, subclave, valor)\n",
    "lista_tuplas = [(k, sk, v) for k, subdict in dict_var_opts.items() for sk, v in subdict.items()]\n",
    "# Crear un DataFrame a partir de la lista de tuplas\n",
    "df_catalogo_respuestas = pd.DataFrame(lista_tuplas, columns=['pregunta', 'id_respuesta', 'respuesta'])\n",
    "df_catalogo_respuestas['id_respuesta']= df_catalogo_respuestas['id_respuesta'].astype(np.int64)\n",
    "df_catalogo_respuestas['pregunta']= df_catalogo_respuestas['pregunta'].str.lower()\n",
    "df_catalogo_respuestas['respuesta']= df_catalogo_respuestas['respuesta'].str.lower()\n",
    "\n",
    "#Catálogo de metadata de la metadata de preguntas y respuestas\n",
    "\n",
    "df_dict_cols_names=dict_to_df(lst_meta_dicts[0])\n",
    "df_dict_labels_type=dict_to_df(lst_meta_dicts[1])\n",
    "df_dict_redstat_type=dict_to_df(lst_meta_dicts[2])\n",
    "df_dict_var2label=dict_to_df(lst_meta_dicts[3])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dbpm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
